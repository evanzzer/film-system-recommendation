# -*- coding: utf-8 -*-
"""Film Recommendation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xXKWzIIxkwqzkTO-Zre3l6ErWsRPXXg7

# Movie Recommendation

## About the Dataset

These files contain metadata for all 45,000 movies listed in the Full MovieLens Dataset. The dataset consists of movies released on or before July 2017. Data points include cast, crew, plot keywords, budget, revenue, posters, release dates, languages, production companies, countries, TMDB vote counts and vote averages.

This dataset also has files containing 26 million ratings from 270,000 users for all 45,000 movies. Ratings are on a scale of 1-5 and have been obtained from the official GroupLens website.

## File that will be used

- `movies_metadata.csv`: The main Movies Metadata file. Contains information on 45,000 movies featured in the Full MovieLens dataset. Features include posters, backdrops, budget, revenue, release dates, languages, production countries and companies.
- `ratings_small.csv`: The subset of 100,000 ratings from 700 users on 9,000 movies. (Original Rating Dataset has 26 millions)

## Data Preparation

Datasets are first fetched from Kaggle
"""

from google.colab import drive
drive.mount('/content/drive')

!mkdir -pv /root/.kaggle
!cp -v /content/drive/MyDrive/kaggle.json /root/.kaggle
!kaggle datasets download -d rounakbanik/the-movies-dataset

!unzip /content/the-movies-dataset.zip

"""Next, we put `movies_metadata.csv` and `ratings_small.csv` to dataframe."""

import pandas as pd 

movie  = pd.read_csv('movies_metadata.csv')
rating = pd.read_csv('ratings_small.csv')

"""Now we check `movies_metadata.csv` and `ratings_small.csv` column information"""

movie.info()

rating.info()

"""Check if the data is null"""

movie.isnull().sum()

rating.isnull().sum()

"""## Data Processing

We will be using Collaborative Learning. Hence, we will only include important rows.

From `movie` dataframe, we will be using `id`, `genres`, and `title`.

From `rating` dataframe, we will be using `userId`, `movieId`, and `rating`.
"""

movie = movie[['id', 'title', 'genres']]
movie.head()

rating = rating[['userId', 'movieId', 'rating']]
rating.head()

"""The `genres` is in JSON format and we only want to extract the `name` part. Hence, we will be converting from JSON into String"""

import json

def convertGenre(string):
    genres = json.loads(string.replace("'", '"'))
    parsed_genres = [genre["name"] for genre in genres]
    return ", ".join(parsed_genres)

genres = movie.pop('genres').tolist()
movie['genres'] = [convertGenre(x) for x in genres]
movie.head()

"""Check movie total data and movie unique data"""

print(f"Total Data : {len(movie)}")
print(f"Unique Data: {len(movie.id.unique())}")

"""There is some duplicates in the data. Therefore, we need to clean the duplicates."""

movie = movie.drop_duplicates('id')
print(f"Total Unique Data after cleaning duplicates: {len(movie)}")

"""Now we want to list movie that is also exist in Rating, and vice versa. Therefore, we want to filter so that `movieId` in `rating` and `id` in `movie` are both exists."""

# Filter movie dataset so that only movieId in rating is exist in movie dataset
movie = movie[movie['id'].isin(rating.movieId.map(lambda x: str(x)))]
print(f"Filtered Movie Dataset : {movie.shape[0]}")

# Filter rating dataset so that only id in movie is exist in rating dataset
rating = rating[rating['movieId'].isin(movie.id.map(lambda x: int(x)))]
print(f"Filtered Rating Dataset: {rating.shape[0]}")

"""The data is now containing only IDs that we want to process. Now we can proceed into developing the model by using Collaborative Learning.

## Model Development

Before we model our data, we need to process everything to make sure the model can be made. We first process all unique values in `userId` and `movieId`
"""

# Make list of Unique User ID
user_ids = rating['userId'].unique().tolist()

print(f"List  ID: {user_ids}")
print(f"Total ID: {len(user_ids)}")

# Encoded User ID
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}

# Decoded User ID
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}

"""Encoding `userId` is done. Next, we will be processing `movieId`."""

# Make list of Unique Movie ID
movie_ids = rating['movieId'].map(lambda x: str(x)).unique().tolist()
print(f"List  ID: {movie_ids}")
print(f"Total ID: {len(movie_ids)}")

# Encoded Movie ID
movie_to_movie_encoded = {x: i for i, x in enumerate(movie_ids)}

# Decoded Movie ID
movie_encoded_to_movie = {i: x for i, x in enumerate(movie_ids)}

"""Next, we map the `userId` and `movieId` to the dataframe by mapping them with the encoded respective code."""

rating['user'] = rating['userId'].map(user_to_user_encoded)
rating['movie'] = rating['movieId'].map(lambda x: str(x)).map(movie_to_movie_encoded)

"""Now we want to find out the minimum and the maximum of the rating."""

min_rating = min(rating['rating'])
max_rating = max(rating['rating'])
print(f"Min Rating: {min_rating}, Max Rating: {max_rating}")

"""Now we can determine the x and the y values."""

x = rating[['user', 'movie']].values
y = rating['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

"""We split the train and test data into 4:1 proportions."""

train_indices = int(0.8 * rating.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)
 
print(x, y)

"""The data is now ready to be modeled. Next, we move on to the training process.

## Training

We first define the layers that will be needed for training
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

class RecommenderNet(tf.keras.Model):
 
    # Insialisasi fungsi
    def __init__(self, num_users, num_movie, embedding_size, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)
        self.num_users = num_users
        self.num_movie = num_movie
        self.embedding_size = embedding_size

        # User Embedded Layer
        self.user_embedding = layers.Embedding( 
            num_users,
            embedding_size,
            embeddings_initializer = 'he_normal',
            embeddings_regularizer = keras.regularizers.l2(1e-6)
        )
        self.user_bias = layers.Embedding(num_users, 1)   # User Embedded Layer Bias

        # Movie Embedded Layer
        self.movie_embedding = layers.Embedding(
            num_movie,
            embedding_size,
            embeddings_initializer = 'he_normal',
            embeddings_regularizer = keras.regularizers.l2(1e-6)
        )
        self.movie_bias = layers.Embedding(num_movie, 1)  # Movie Embedded Layer Bias
 
    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:,0])    # Call Embedded Layer #1
        user_bias = self.user_bias(inputs[:, 0])          # Call Embedded Layer #2
        movie_vector = self.movie_embedding(inputs[:, 1]) # Call Embedded Layer #3
        movie_bias = self.movie_bias(inputs[:, 1])        # Call Embedded Layer #4
    
        dot_user_movie = tf.tensordot(user_vector, movie_vector, 2) 
    
        x = dot_user_movie + user_bias + movie_bias
        
        return tf.nn.sigmoid(x) # sigmoid activation

"""Now we compile our model with `BinaryCrossentropy()` Loss Function, `Adam()` optimizer with `lr=1e-3` and `RMS Error` metrics."""

model = RecommenderNet(len(user_ids), len(movie_ids), 50)

model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3),
    metrics = [tf.keras.metrics.RootMeanSquaredError()]
)

"""Now we can start our training"""

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 32,
    epochs = 100,
    validation_data = (x_val, y_val)
)

"""## Visualization

We have done training the model. We can visualize our result in a line chart.
"""

import matplotlib.pyplot as plt

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('Model Metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""## Model Testing

It is time that we try our model. First we prepare a user sample and all the informations needed for that specific user.
"""

import numpy as np

# Get User Sample
user_id = rating.userId.sample(1).iloc[0]

# Get Movies that has been watched by the user
movie_watched_by_user = rating[rating.userId == user_id]
print(movie_watched_by_user)
 
# Get Movies that have not been watched by the user
movie_not_watched = movie[(~movie['id'].isin(movie_watched_by_user.movieId.map(lambda x: str(x)).values))]['id']
movie_not_watched = list(
    set(movie_not_watched).intersection(set(movie_to_movie_encoded.keys()))
)
 
# Encode the movie
movie_not_watched = [[movie_to_movie_encoded.get(x)] for x in movie_not_watched]
# Encode the user
user_encoder = user_to_user_encoded.get(user_id)

user_movie_array = np.hstack(
    ([[user_encoder]] * len(movie_not_watched), movie_not_watched)
)
print(user_movie_array)

"""Finally, we can retrive the recommended movie for the specified user"""

ratings = model.predict(user_movie_array).flatten()
 
top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_movie_ids = [
    movie_encoded_to_movie.get(movie_not_watched[x][0]) for x in top_ratings_indices
]
 
print(f'Showing recommendations for users: {user_id}')
print('====' * 8)
print('Highest Movie Rating from User:')
print('----' * 8)
 
top_movie_user = (
    movie_watched_by_user.sort_values(
        by = 'rating',
        ascending=False
    ).head(5).movieId.map(lambda x: str(x)).values
)

 
movie_rows = movie[movie['id'].isin(top_movie_user)][:5]
for row in movie_rows.itertuples():
    print(row.title, ':', row.genres)
 
print('----' * 8)
print('Top 10 Movie Recommendation')
print('----' * 8)
 
recommended_movie = movie[movie['id'].isin(recommended_movie_ids)]
for row in recommended_movie.itertuples():
    print(row.title, ':', row.genres)